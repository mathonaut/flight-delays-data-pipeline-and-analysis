{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d86d33-181e-40d2-a0fe-90c4548220af",
   "metadata": {},
   "source": [
    "# etl_silver_to_gold\n",
    "---\n",
    "Este notebook executa o processo `ETL` que transfere os dados da camada **Silver** para a **Gold**, englobando normalização, movimentação dos arquivos e carga dos dados no *PostgreSQL*, dando finalidade ao pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ebf692-724f-4183-9db8-f827aa80f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "run_mode = \"latest\"\n",
    "run_date = None\n",
    "\n",
    "silver_path = \"/opt/airflow/data-layer/silver\"\n",
    "gold_path = \"/opt/airflow/data-layer/gold\"\n",
    "\n",
    "aggregated_name = \"flights_aggregated.parquet\"\n",
    "postgres_conn_id = \"AIRFLOW_VAR_POSTGRES_CONN_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90039f5-ae72-4d8b-97e4-0ba25c56ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from transformer.utils.file_io import find_partition\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.spark_helpers import get_spark_session, load_to_postgres, read_from_postgres\n",
    "from transformer.utils.postgre_helpers import assert_table_rowcount\n",
    "from transformer.utils.quality_gates_gold import run_quality_gates_gold\n",
    "\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import DateType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdbbf0-e811-40f5-8a53-c08b22d520e7",
   "metadata": {},
   "source": [
    "## Job 1: build_and_load_gold_star_schema\n",
    "\n",
    "Este job realiza a construção do esquema estrela da camada **Gold**, materializando as tabelas dimensionais e fato a partir da tabela `silver_flights`, salva os dados em formato `parquet` na camada **Gold** e carregando os dados no *PostgreSQL* de acordo com o ddl da camada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97709fef-fbb7-492c-9ebb-01d905ec42f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c9643096-85a2-4dc2-acab-117d373dfd24;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 160ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c9643096-85a2-4dc2-acab-117d373dfd24\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n",
      "25/12/11 02:46:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-12-11 02:46:29 [INFO] spark_helpers: [INFO] SparkSession criada: 'BuildLoadGoldStarSchema' (master=local[*]).\n",
      "2025-12-11 02:46:29 [INFO] build_and_load_gold_star_schema: [BuildLoad] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"build_and_load_gold_star_schema\")\n",
    "\n",
    "spark = get_spark_session(\"BuildLoadGoldStarSchema\")\n",
    "log.info(\"[BuildLoad] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228db41-f5f1-4fe7-87f6-60f1faacfadf",
   "metadata": {},
   "source": [
    "### Definindo função de materialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e3b960-7ef3-4f87-a5bb-62ffed79ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def materialize_gold_layer(df: DataFrame) -> dict[str, DataFrame]:\n",
    "    \"\"\"\n",
    "    Materializa as tabelas dimensionais e fato da camada gold a partir do DataFrame agregado da Silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame consolidado da camada Gold.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, DataFrame]: DataFrames correspondentes a dim_air, dim_apt, dim_dat e fat_flt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Feriados federais nos EUA em 2015 (UTC)\n",
    "    us_holidays_2015 = [\n",
    "        \"2015-01-01\",\n",
    "        \"2015-01-19\",\n",
    "        \"2015-02-16\",\n",
    "        \"2015-05-25\",\n",
    "        \"2015-07-04\",\n",
    "        \"2015-09-07\",\n",
    "        \"2015-10-12\",\n",
    "        \"2015-11-11\",\n",
    "        \"2015-11-26\",\n",
    "        \"2015-12-25\",\n",
    "    ]\n",
    "    holidays_df = (\n",
    "        spark.createDataFrame([(d,) for d in us_holidays_2015], [\"holiday_date\"])\n",
    "            .withColumn(\"holiday_date\", F.col(\"holiday_date\").cast(DateType()))\n",
    "    )\n",
    "    \n",
    "    log.info(\"[Materialize] Iniciando materialização da camada Gold.\")\n",
    "\n",
    "    # dim_air\n",
    "    log.info(\"[Materialize] Materializando 'dim_air'.\")\n",
    "    w_air = Window.orderBy(\"air_iat\")\n",
    "    dim_air = (\n",
    "        df.select(\n",
    "            F.col(\"airline_iata_code\").alias(\"air_iat\"),\n",
    "            F.col(\"airline_name\").alias(\"air_nam\")\n",
    "        )\n",
    "        .distinct()\n",
    "        .withColumn(\"srk_air\", F.row_number().over(w_air))\n",
    "        .select(\"srk_air\", \"air_iat\", \"air_nam\")\n",
    "    )\n",
    "\n",
    "    # dim_apt\n",
    "    log.info(\"[Materialize] Materializando 'dim_apt'.\")\n",
    "    \n",
    "    state_map = F.create_map(\n",
    "    \t[F.lit(x) for x in [\n",
    "    \t\t\"AL\",\"Alabama\",\"AK\",\"Alaska\",\"AZ\",\"Arizona\",\"AR\",\"Arkansas\",\"CA\",\"California\",\n",
    "        \t\"CO\",\"Colorado\",\"CT\",\"Connecticut\",\"DE\",\"Delaware\",\"FL\",\"Florida\",\"GA\",\"Georgia\",\n",
    "        \t\"HI\",\"Hawaii\",\"ID\",\"Idaho\",\"IL\",\"Illinois\",\"IN\",\"Indiana\",\"IA\",\"Iowa\",\"KS\",\"Kansas\",\n",
    "        \t\"KY\",\"Kentucky\",\"LA\",\"Louisiana\",\"ME\",\"Maine\",\"MD\",\"Maryland\",\"MA\",\"Massachusetts\",\n",
    "        \t\"MI\",\"Michigan\",\"MN\",\"Minnesota\",\"MS\",\"Mississippi\",\"MO\",\"Missouri\",\"MT\",\"Montana\",\n",
    "        \t\"NE\",\"Nebraska\",\"NV\",\"Nevada\",\"NH\",\"New Hampshire\",\"NJ\",\"New Jersey\",\"NM\",\"New Mexico\",\n",
    "        \t\"NY\",\"New York\",\"NC\",\"North Carolina\",\"ND\",\"North Dakota\",\"OH\",\"Ohio\",\"OK\",\"Oklahoma\",\n",
    "        \t\"OR\",\"Oregon\",\"PA\",\"Pennsylvania\",\"RI\",\"Rhode Island\",\"SC\",\"South Carolina\",\n",
    "        \t\"SD\",\"South Dakota\",\"TN\",\"Tennessee\",\"TX\",\"Texas\",\"UT\",\"Utah\",\"VT\",\"Vermont\",\n",
    "        \t\"VA\",\"Virginia\",\"WA\",\"Washington\",\"WV\",\"West Virginia\",\"WI\",\"Wisconsin\",\"WY\",\"Wyoming\",\n",
    "        \t\"DC\",\"District of Columbia\"\n",
    "   \t \t]]\n",
    "    )\n",
    "    \n",
    "    apt_base = (\n",
    "        df.select(\n",
    "            F.col(\"origin_airport_iata_code\").alias(\"apt_iat\"),\n",
    "            F.col(\"origin_airport_name\").alias(\"apt_nam\"),\n",
    "            F.col(\"origin_state\").alias(\"ste_cod\"),\n",
    "            F.coalesce(state_map[F.col(\"origin_state\")], F.lit(\"Unknown\")).alias(\"ste_nam\"),\n",
    "            F.col(\"origin_city\").alias(\"cty_nam\"),\n",
    "            F.col(\"origin_latitude\").alias(\"lat_val\"),\n",
    "            F.col(\"origin_longitude\").alias(\"lon_val\")\n",
    "        )\n",
    "        .union(\n",
    "            df.select(\n",
    "                F.col(\"dest_airport_iata_code\").alias(\"apt_iat\"),\n",
    "                F.col(\"dest_airport_name\").alias(\"apt_nam\"),\n",
    "                F.col(\"dest_state\").alias(\"ste_cod\"),\n",
    "                F.coalesce(state_map[F.col(\"dest_state\")], F.lit(\"Unknown\")).alias(\"ste_nam\"),\n",
    "                F.col(\"dest_city\").alias(\"cty_nam\"),\n",
    "                F.col(\"dest_latitude\").alias(\"lat_val\"),\n",
    "                F.col(\"dest_longitude\").alias(\"lon_val\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    w_apt = Window.orderBy(\"apt_iat\")\n",
    "    dim_apt = (\n",
    "        apt_base.distinct()\n",
    "        .withColumn(\"srk_apt\", F.row_number().over(w_apt))\n",
    "        .select(\n",
    "            \"srk_apt\", \"apt_iat\", \"apt_nam\",\n",
    "            \"ste_cod\", \"ste_nam\", \"cty_nam\",\n",
    "            \"lat_val\", \"lon_val\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # dim_dat\n",
    "    log.info(\"[Materialize] Materializando 'dim_dat'.\")\n",
    "    dat_base = (\n",
    "        df.select(F.col(\"flight_date\").alias(\"ful_dat\"))\n",
    "          .distinct()\n",
    "          .withColumn(\"yer\", F.year(\"ful_dat\"))\n",
    "          .withColumn(\"mth\", F.month(\"ful_dat\"))\n",
    "          .withColumn(\"day\", F.dayofmonth(\"ful_dat\"))\n",
    "          .withColumn(\"dow\", F.dayofweek(\"ful_dat\"))\n",
    "          .withColumn(\"qtr\", F.quarter(\"ful_dat\"))\n",
    "          .join(holidays_df, F.col(\"ful_dat\") == F.col(\"holiday_date\"), \"left\")\n",
    "          .withColumn(\"hol_flg\", F.col(\"holiday_date\").isNotNull())\n",
    "          .drop(\"holiday_date\")\n",
    "    )\n",
    "\n",
    "    w_dat = Window.orderBy(\"ful_dat\")\n",
    "    dim_dat = (\n",
    "        dat_base.distinct()\n",
    "          .withColumn(\"srk_dat\", F.row_number().over(w_dat))\n",
    "          .select(\"srk_dat\", \"ful_dat\", \"yer\", \"mth\", \"day\", \"dow\", \"qtr\", \"hol_flg\")\n",
    "    )\n",
    "\n",
    "    # Adiciona surrogate keys na fat_flt\n",
    "    fat = (\n",
    "        df\n",
    "        .withColumnRenamed(\"flight_date\", \"ful_dat\")\n",
    "        .join(\n",
    "            dim_air.select(\"srk_air\", \"air_iat\"),\n",
    "            df.airline_iata_code == F.col(\"air_iat\"), \"left\")\n",
    "        .join(\n",
    "            dim_apt.select(\n",
    "                F.col(\"srk_apt\").alias(\"srk_ori\"),\n",
    "                F.col(\"apt_iat\").alias(\"ori_iat\")\n",
    "            ),\n",
    "            df.origin_airport_iata_code == F.col(\"ori_iat\"),\n",
    "            \"left\"\n",
    "        )\n",
    "        .join(\n",
    "            dim_apt.select(\n",
    "                F.col(\"srk_apt\").alias(\"srk_dst\"),\n",
    "                F.col(\"apt_iat\").alias(\"dst_iat\")\n",
    "            ),\n",
    "            df.dest_airport_iata_code == F.col(\"dst_iat\"),\n",
    "            \"left\"\n",
    "        )\n",
    "        .join(\n",
    "            dim_dat.select(\"srk_dat\", \"ful_dat\"),\n",
    "            \"ful_dat\",\n",
    "            \"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Gera a sk sequencial para fat_flt\n",
    "    w_flt = Window.orderBy(\"ful_dat\", \"airline_iata_code\", \"origin_airport_iata_code\",\n",
    "                           \"dest_airport_iata_code\", \"scheduled_departure\")\n",
    "\n",
    "    fat_flt = (\n",
    "        fat.withColumn(\"srk_flt\", F.row_number().over(w_flt))\n",
    "           .select(\n",
    "               \"srk_flt\", \"srk_dat\", \"srk_air\", \"srk_ori\", \"srk_dst\",\n",
    "               F.col(\"scheduled_departure\").alias(\"sch_dep\"),\n",
    "               F.col(\"departure_time\").alias(\"dep_tme\"),\n",
    "               F.col(\"scheduled_arrival\").alias(\"sch_arr\"),\n",
    "               F.col(\"arrival_time\").alias(\"arr_tme\"),\n",
    "               F.col(\"distance\").alias(\"dis_val\"),\n",
    "               F.col(\"air_time\").alias(\"air_tme\"),\n",
    "               F.col(\"elapsed_time\").alias(\"elp_tme\"),\n",
    "               F.col(\"scheduled_time\").alias(\"sch_tme\"),\n",
    "               F.col(\"departure_delay\").alias(\"dep_dly\"),\n",
    "               F.col(\"arrival_delay\").alias(\"arr_dly\"),\n",
    "               F.col(\"air_system_delay\").alias(\"sys_dly\"),\n",
    "               F.col(\"security_delay\").alias(\"sec_dly\"),\n",
    "               F.col(\"airline_delay\").alias(\"air_dly\"),\n",
    "               F.col(\"late_aircraft_delay\").alias(\"acf_dly\"),\n",
    "               F.col(\"weather_delay\").alias(\"wea_dly\"),\n",
    "               F.col(\"is_overnight_flight\").alias(\"ovn_flg\")\n",
    "           )\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"dim_air\": dim_air,\n",
    "        \"dim_apt\": dim_apt,\n",
    "        \"dim_dat\": dim_dat,\n",
    "        \"fat_flt\": fat_flt\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b51023-792b-4e73-908f-dcd02c7cac2c",
   "metadata": {},
   "source": [
    "### Runner para o job `build_and_load_gold_star_schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedf860a-e188-4e9d-8f11-b6875777dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 02:46:29 [INFO] build_and_load_gold_star_schema: [BuildLoad] Iniciando job de materialização da gold.\n",
      "2025-12-11 02:46:29 [INFO] spark_helpers: [READ] Iniciando leitura de 'silver.silver_flights'.\n",
      "2025-12-11 02:46:29 [WARN] spark_helpers: [WARN] Airflow indisponível, usando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-12-11 02:46:34 [INFO] spark_helpers: [READ] Leitura concluída: 'silver.silver_flights'. Linhas: 5208259\n",
      "2025-12-11 02:46:34 [INFO] build_and_load_gold_star_schema: [BuildLoad] Datasets carregado a partir do PostgreSQL.\n",
      "2025-12-11 02:46:34 [INFO] build_and_load_gold_star_schema: [Materialize] Iniciando materialização da camada Gold.\n",
      "2025-12-11 02:46:34 [INFO] build_and_load_gold_star_schema: [Materialize] Materializando 'dim_air'.\n",
      "2025-12-11 02:46:34 [INFO] build_and_load_gold_star_schema: [Materialize] Materializando 'dim_apt'.\n",
      "2025-12-11 02:46:35 [INFO] build_and_load_gold_star_schema: [Materialize] Materializando 'dim_dat'.\n",
      "2025-12-11 02:46:35 [INFO] build_and_load_gold_star_schema: [BuildLoad] Iniciando quality gate.\n",
      "2025-12-11 02:46:35 [INFO] quality_gates_gold: [Quality][Gold] Iniciando validações.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:46:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:47:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:47:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:47:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:47:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2025-12-11 02:47:49 [INFO] quality_gates_gold: [Quality][Gold]      _check_unique: 'air_iat' OK.\n",
      "25/12/11 02:49:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2025-12-11 02:49:07 [INFO] quality_gates_gold: [Quality][Gold]      _check_unique: 'apt_iat' OK.\n",
      "25/12/11 02:49:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2025-12-11 02:49:17 [INFO] quality_gates_gold: [Quality][Gold]      _check_unique: 'ful_dat' OK.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:49:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:51:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:51:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:51:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/11 02:51:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2025-12-11 02:51:36 [INFO] quality_gates_gold: [Quality][Gold]      _check_unique: 'srk_flt' OK.\n",
      "2025-12-11 02:51:36 [INFO] quality_gates_gold: [Quality][Gold]      _check_no_nulls: fat_flt OK.\n",
      "2025-12-11 02:51:38 [INFO] quality_gates_gold: [Quality][Gold]           _check_fk_integrity: [fat_flt] 'srk_air' <-> 'dim_air.srk_air' OK.\n",
      "2025-12-11 02:51:39 [INFO] quality_gates_gold: [Quality][Gold]           _check_fk_integrity: [fat_flt] 'srk_ori' <-> 'dim_apt.srk_apt' OK.\n",
      "2025-12-11 02:51:39 [INFO] quality_gates_gold: [Quality][Gold]           _check_fk_integrity: [fat_flt] 'srk_dst' <-> 'dim_apt.srk_apt' OK.\n",
      "2025-12-11 02:51:40 [INFO] quality_gates_gold: [Quality][Gold]           _check_fk_integrity: [fat_flt] 'srk_dat' <-> 'dim_dat.srk_dat' OK.\n",
      "2025-12-11 02:51:40 [INFO] quality_gates_gold: [Quality][Gold] Todas as validações concluídas com sucesso.\n",
      "2025-12-11 02:51:40 [INFO] build_and_load_gold_star_schema: [BuildLoad] Quality gate concluído com sucesso.\n",
      "2025-12-11 02:51:40 [INFO] build_and_load_gold_star_schema: [BuildLoad] Iniciando carga da gold.\n",
      "2025-12-11 02:51:40 [INFO] build_and_load_gold_star_schema: [BuildLoad] Carregando tabela: gold.dim_air.\n",
      "2025-12-11 02:51:40 [WARN] spark_helpers: [WARN] Airflow indisponível, usando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-12-11 02:51:40 [INFO] spark_helpers: [LOAD] Limpando tabela 'gold.dim_air' (TRUNCATE).\n",
      "2025-12-11 02:51:43 [INFO] spark_helpers: [LOAD] Carga concluída em 'gold.dim_air' (modo=append).\n",
      "2025-12-11 02:51:43 [INFO] build_and_load_gold_star_schema: [BuildLoad] Tabela 'gold.dim_air' carregada. Validando integridade.\n",
      "2025-12-11 02:51:43 [INFO] postgres_helpers: [AssertRowCount] Validando contagem da tabela 'gold.dim_air'.\n",
      "2025-12-11 02:51:43 [INFO] postgres_helpers: [AssertRowCount] Esperado: 14 | Encontrado: 14\n",
      "2025-12-11 02:51:43 [INFO] postgres_helpers: [AssertRowCount] Validação concluída com sucesso.\n",
      "2025-12-11 02:51:43 [INFO] build_and_load_gold_star_schema: [BuildLoad] Validação concluída com sucesso: gold.dim_air.\n",
      "2025-12-11 02:51:43 [INFO] build_and_load_gold_star_schema: [BuildLoad] Carregando tabela: gold.dim_apt.\n",
      "2025-12-11 02:51:43 [WARN] spark_helpers: [WARN] Airflow indisponível, usando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-12-11 02:51:43 [INFO] spark_helpers: [LOAD] Limpando tabela 'gold.dim_apt' (TRUNCATE).\n",
      "2025-12-11 02:51:44 [INFO] spark_helpers: [LOAD] Carga concluída em 'gold.dim_apt' (modo=append).\n",
      "2025-12-11 02:51:44 [INFO] build_and_load_gold_star_schema: [BuildLoad] Tabela 'gold.dim_apt' carregada. Validando integridade.\n",
      "2025-12-11 02:51:44 [INFO] postgres_helpers: [AssertRowCount] Validando contagem da tabela 'gold.dim_apt'.\n",
      "2025-12-11 02:51:44 [INFO] postgres_helpers: [AssertRowCount] Esperado: 322 | Encontrado: 322\n",
      "2025-12-11 02:51:44 [INFO] postgres_helpers: [AssertRowCount] Validação concluída com sucesso.\n",
      "2025-12-11 02:51:44 [INFO] build_and_load_gold_star_schema: [BuildLoad] Validação concluída com sucesso: gold.dim_apt.\n",
      "2025-12-11 02:51:44 [INFO] build_and_load_gold_star_schema: [BuildLoad] Carregando tabela: gold.dim_dat.\n",
      "2025-12-11 02:51:44 [WARN] spark_helpers: [WARN] Airflow indisponível, usando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-12-11 02:51:44 [INFO] spark_helpers: [LOAD] Limpando tabela 'gold.dim_dat' (TRUNCATE).\n",
      "2025-12-11 02:51:44 [INFO] spark_helpers: [LOAD] Carga concluída em 'gold.dim_dat' (modo=append).\n",
      "2025-12-11 02:51:44 [INFO] build_and_load_gold_star_schema: [BuildLoad] Tabela 'gold.dim_dat' carregada. Validando integridade.\n",
      "2025-12-11 02:51:44 [INFO] postgres_helpers: [AssertRowCount] Validando contagem da tabela 'gold.dim_dat'.\n",
      "2025-12-11 02:51:44 [INFO] postgres_helpers: [AssertRowCount] Esperado: 334 | Encontrado: 334\n",
      "2025-12-11 02:51:44 [INFO] postgres_helpers: [AssertRowCount] Validação concluída com sucesso.\n",
      "2025-12-11 02:51:44 [INFO] build_and_load_gold_star_schema: [BuildLoad] Validação concluída com sucesso: gold.dim_dat.\n",
      "2025-12-11 02:51:44 [INFO] build_and_load_gold_star_schema: [BuildLoad] Carregando tabela: gold.fat_flt.\n",
      "2025-12-11 02:51:45 [WARN] spark_helpers: [WARN] Airflow indisponível, usando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-12-11 02:51:45 [INFO] spark_helpers: [LOAD] Limpando tabela 'gold.fat_flt' (TRUNCATE).\n",
      "25/12/11 02:51:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2025-12-11 02:59:17 [INFO] spark_helpers: [LOAD] Carga concluída em 'gold.fat_flt' (modo=append).\n",
      "2025-12-11 02:59:17 [INFO] build_and_load_gold_star_schema: [BuildLoad] Tabela 'gold.fat_flt' carregada. Validando integridade.\n",
      "2025-12-11 02:59:17 [INFO] postgres_helpers: [AssertRowCount] Validando contagem da tabela 'gold.fat_flt'.\n",
      "2025-12-11 02:59:41 [INFO] postgres_helpers: [AssertRowCount] Esperado: 5,208,259 | Encontrado: 5,208,259\n",
      "2025-12-11 02:59:41 [INFO] postgres_helpers: [AssertRowCount] Validação concluída com sucesso.\n",
      "2025-12-11 02:59:41 [INFO] build_and_load_gold_star_schema: [BuildLoad] Validação concluída com sucesso: gold.fat_flt.\n",
      "2025-12-11 02:59:41 [INFO] build_and_load_gold_star_schema: [BuildLoad] Carga de todas as tabelas concluída com sucesso.\n",
      "2025-12-11 02:59:41 [INFO] build_and_load_gold_star_schema: [BuildLoad] Job de materialização da gold encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[BuildLoad] Iniciando job de materialização da gold.\")\n",
    "\n",
    "    df = read_from_postgres(\n",
    "        spark=spark,\n",
    "        db_conn_id=postgres_conn_id,\n",
    "        table_name=\"silver.silver_flights\",\n",
    "    )\n",
    "\n",
    "    log.info(f\"[BuildLoad] Datasets carregado a partir do PostgreSQL.\")\n",
    "\n",
    "    # Materializando\n",
    "    tables = materialize_gold_layer(df)\n",
    "    dim_air = tables[\"dim_air\"]\n",
    "    dim_apt = tables[\"dim_apt\"]\n",
    "    dim_dat = tables[\"dim_dat\"]\n",
    "    fat_flt = tables[\"fat_flt\"]\n",
    "\n",
    "    # Executa quality gate\n",
    "    log.info(\"[BuildLoad] Iniciando quality gate.\")\n",
    "    run_quality_gates_gold(\n",
    "        dim_air=dim_air,\n",
    "        dim_apt=dim_apt,\n",
    "        dim_dat=dim_dat,\n",
    "        fat_flt=fat_flt\n",
    "    )\n",
    "    log.info(\"[BuildLoad] Quality gate concluído com sucesso.\")\n",
    "\n",
    "    \"\"\"\n",
    "    # *** DEBUG ***\n",
    "    # Define partição de saída\n",
    "    processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    output_dir = Path(gold_path) / processing_date / \"PARQUET\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Escreve os arquivos na gold\n",
    "    log.info(\"[BuildLoad] Iniciando escrita dos arquivos na camada gold.\")\n",
    "\n",
    "    dim_air.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_air.parquet\"))\n",
    "    dim_apt.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_apt.parquet\"))\n",
    "    dim_dat.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_dat.parquet\"))\n",
    "    fat_flt.write.mode(\"overwrite\").parquet(str(output_dir / \"fat_flt.parquet\"))\n",
    "\n",
    "    log.info(\"[BuildLoad] Escrita concluída com sucesso.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    log.info(\"[BuildLoad] Iniciando carga da gold.\")\n",
    "\n",
    "    tables = {\n",
    "        \"dim_air\": dim_air,\n",
    "        \"dim_apt\": dim_apt,\n",
    "        \"dim_dat\": dim_dat,\n",
    "        \"fat_flt\": fat_flt,\n",
    "    }\n",
    "\n",
    "    # Carga no PostgreSQL e validação\n",
    "    for table_name, df in tables.items():\n",
    "        full_table_name = f\"gold.{table_name}\"\n",
    "\n",
    "        log.info(f\"[BuildLoad] Carregando tabela: {full_table_name}.\")\n",
    "        expected_count = df.count()\n",
    "\n",
    "        # Carga no PostgreSQL\n",
    "        load_to_postgres(\n",
    "            df=df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=full_table_name,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "\n",
    "        log.info(f\"[BuildLoad] Tabela '{full_table_name}' carregada. Validando integridade.\")\n",
    "\n",
    "        # Validação (fallback se falhar)\n",
    "        try:\n",
    "            assert_table_rowcount(\n",
    "                db_conn_id=postgres_conn_id,\n",
    "                table_name=full_table_name,\n",
    "                expected_count=expected_count,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log.error(f\"[BuildLoad][ERROR] Validação falhou para '{full_table_name}'. Limpando tabela.\")\n",
    "\n",
    "            import psycopg2\n",
    "\n",
    "            with psycopg2.connect(\n",
    "                host=os.getenv(\"DB_HOST\", \"localhost\"),\n",
    "                dbname=os.getenv(\"DB_NAME\", \"postgres\"),\n",
    "                user=os.getenv(\"DB_USER\", \"postgres\"),\n",
    "                password=os.getenv(\"DB_PASSWORD\", \"postgres\"),\n",
    "            ) as conn_pg:\n",
    "                with conn_pg.cursor() as cur:\n",
    "                    cur.execute(f\"TRUNCATE TABLE {full_table_name} CASCADE;\")\n",
    "                    conn_pg.commit()\n",
    "\n",
    "            raise ValueError(f\"[BuildLoad][ERROR] Falha na validação da tabela '{full_table_name}'.\") from e\n",
    "\n",
    "        log.info(f\"[BuildLoad] Validação concluída com sucesso: {full_table_name}.\")\n",
    "\n",
    "    log.info(\"[BuildLoad] Carga de todas as tabelas concluída com sucesso.\") \n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[BuildLoad][ERROR] Falha na construção do esquema estrela: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[BuildLoad] Job de materialização da gold encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8a5123-d9eb-4353-84ac-3fedeed12803",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "# Verifica arquivos\n",
    "\n",
    "df_show = {\n",
    "    \"dim_air\": dim_air,\n",
    "    \"dim_apt\": dim_apt,\n",
    "    \"dim_dat\": dim_dat,\n",
    "    \"fat_flt\": fat_flt\n",
    "}\n",
    "\n",
    "for name, d in df_show.items():\n",
    "    print(f\"\\n{name}\\n\")\n",
    "    d.printSchema()\n",
    "    d.limit(1).show(truncate=True)\n",
    "\n",
    "# Verifica tabelas\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{os.getenv('DB_HOST', 'localhost')}:{os.getenv('DB_PORT', '5432')}/{os.getenv('DB_NAME', 'postgres')}\"\n",
    "connection_properties = {\n",
    "    \"user\": os.getenv(\"DB_USER\", \"postgres\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"postgres\"),\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "tables_to_check = [\"dim_air\", \"dim_apt\", \"dim_dat\"]\n",
    "for tbl in tables_to_check:\n",
    "    print(f\"\\n gold.{tbl} \\n\")\n",
    "    df_check = spark.read.jdbc(url=jdbc_url, table=f\"gold.{tbl}\", properties=connection_properties)\n",
    "    df_check.limit(1).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c38424-aa86-46e4-8f6e-65e0e532c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 03:00:18 [INFO] build_and_load_gold_star_schema: [BuildLoad] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerrando a sessão do Spark.\n",
    "spark.stop()\n",
    "log.info(\"[BuildLoad] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b070f4-6652-4d0c-96cf-5cb8f77ac23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
